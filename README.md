# :fire: FirePy Project :fire:

## :mage: Context :mage:
![Datascientest_logo](https://github.com/DataScientest-Studio/firepy/blob/main/resources/image6.png)

This project was made out as part of training course at  [Datascientest.com](https://datascientest.com/) (Data Scientist program June '21) .

## :dart: Objectif :dart:
Create a deep learning model based on U-Net architecture for semantic segmentation of satellite images (Sentinel-2) to detect burned area.

## :office_worker: Business case :office_worker:
Manual surveying of fire perimeters is a tedious and tiring task and subject to human error. The level could be limited due to pockets spared by the flames found inside the fire zones.

The automation of the detection of burnt surfaces by an algorithm would make it possible to lighten the human workload, to bring robustness to the analysis and to scale the analysis area and would make it possible to provide a preliminary estimate of damage (natural, property, infrastructure). Mathematical processing of satellite images is also a way to go beyond burnt/unburnt binary information and refine the burn level.

Traditional burn area detection methods have limited performance. By operating using thresholds, it is difficult for these techniques to detect small burned areas or low intensity burns. Some algorithms based on anomaly detection on the neighborhood of pixels have a high rate of false detection. Finally, methods exploiting differences in image sequences over time have the disadvantage of requiring a lot of data. Faced with this observation, deep learning algorithms seem particularly promising and are the subject of numerous research and publications.

The good availability of data from the Sentinel 2 satellites financed by ESA's Copernicus program is an essential asset for the launch of the project. Indeed, the images are available free of charge and have covered a large part of the earth's surface since 2015.


## :bookmark_tabs: Files Descriptions :bookmark_tabs:
-	1a_data_export.ipynb
-	1b_patches_generation.ipynb
-	2c_Unet_model_training_with_patches&albumentation.ipynb
-	2d_PSPNet_model_training_resize.ipynb.ipynb

## Streamlit demo

![https://share.streamlit.io/icecore013/firepy/main/streamlit/index.py](https://github.com/DataScientest-Studio/firepy/blob/main/resources/streamlit-logo-primary-colormark-darktext.png)

## :floppy_disk: Google Drive :floppy_disk:
The backup files and images used to train/test our models are hosted on Google drive because they are too large to be hosted on GitHub. 

## :computer: Credits :computer:

*	:fairy_woman: Emmanuelle CANO [(LinkedIn)](https://www.linkedin.com/in/emmanuelle-cano-4b845940/)
*	:sunglasses: FranÃ§ois FAUPIN [(LinkedIn)](https://www.linkedin.com/in/francois-faupin/)
*	:zombie_man: Thomas GOSSART [(LinkedIn)](https://www.linkedin.com/in/gossartt/)

Mentor :
*	:genie: Pierre ADEIKALAM [(LinkedIn)](https://www.linkedin.com/in/data-jesus/)








## :heavy_check_mark: MÃ©thodologie et environnement de travail :heavy_check_mark:

Dans la cartographie des techniques de machine learning, le challenge du projet correspond Ã  une tÃ¢che de classification supervisÃ©e et, plus particuliÃ¨rement, de segmentation sÃ©mantique. Il sâ€™agit de classifier chaque pixel dâ€™une image (pixel brÃ»lÃ© / non brÃ»lÃ©).

Afin dâ€™atteindre cet objectif, la mÃ©thodologie globale appliquÃ©e suivra les Ã©tapes ci-dessous:
- Recherche bibliographique sur les mÃ©thodes de dÃ©tection de zones brÃ»lÃ©es
- Constitution dâ€™une base de donnÃ©es suffisamment propre et variÃ©e
- Entrainement dâ€™un algorithme de Deep Learning Ã  partir dâ€™une rÃ©gion pour laquelle des donnÃ©es de vÃ©ritÃ© terrain (informations collectÃ©es par lâ€™homme et non automatisÃ©es) sont disponibles,
- Application de lâ€™algorithme Ã  dâ€™autres scÃ¨nes et dâ€™autres incendies pour produire une cartographie du contour de la zone brÃ»lÃ©e une fois le feu maÃ®trisÃ© par classification au pixel. 

La nature des donnÃ©es dâ€™entrÃ©e a nÃ©cessitÃ© une montÃ©e en compÃ©tences sur les images gÃ©o-rÃ©fÃ©rencÃ©es. En effet, des structures de donnÃ©es particuliÃ¨res (Fichiers vecteur Shapefile, Raster, GeoDataframeâ€¦) et des outils dÃ©diÃ©s (QGIS, Geopandas, Rasterioâ€¦) ont dÃ» Ãªtre mis en Å“uvre.

Enfin, afin de pouvoir travailler en collaboratif sur un environnement permettant les calculs de deep learning, nous avons utilisÃ© les outils Google (Drive, Colab, Google Earth Engine) ainsi quâ€™un espace Github dÃ©diÃ©.


## :satellite: Description des donnÃ©es des satellites Sentinel 2 :satellite:
Les satellites Sentinel 2A et 2B rÃ©alisent des acquisitions dans le domaine optique depuis 2015 (https://fr.wikipedia.org/wiki/Sentinel-2 ). FinancÃ©es par le programme europÃ©en Copernicus, ces missions permettent de produire des images entiÃ¨rement libres sans restriction dâ€™usage ou de profil dâ€™utilisateur (https://sentinel2.cnes.fr/fr).
La rÃ©solution spatiale est de 20 m, une image Sentinel 2 se compose dâ€™un peu plus de 30 millions de pixels.
Lâ€™ensemble des caractÃ©ristiques techniques des images Sentinel 2 est prÃ©sentÃ© ici : 
https://sentinel.esa.int/web/sentinel/technical-guides/sentinel-2-msi/msi-instrument

Les caractÃ©ristiques des bandes spectrales :

Le capteur Sentinel 2 (MSI) permet de rÃ©aliser des acquisitions dans 13 bandes spectrales de rÃ©solutions spatiales diffÃ©rentes (de 10 Ã  60 m) dans les domaines du visible, du proche et du moyen infrarouge (https://sentinel.esa.int/web/sentinel/user-guides/sentinel-2-msi/resolutions/spatial)
Le calcul de lâ€™indice permettant de dÃ©tecter les zones brÃ»lÃ©es fera appel aux bandes 7 (NIR) et 12 (SWIR), toutes les deux Ã  20 m de rÃ©solution spatiale.
On pourra Ã©galement utiliser les bandes du visible et du proche infrarouge Ã  10m (2,3,4 et 8) pour rÃ©aliser des compositions colorÃ©es permettant dâ€™analyser visuellement les territoires Ã  traiter ainsi que de calculer dâ€™autres indices en lien avec la densitÃ© de vÃ©gÃ©tation tels que le NDVI.

![image](https://user-images.githubusercontent.com/31386060/155057380-e61428af-56c1-4fcb-bf75-e28a9e670619.png)
 
## :earth_americas: Description des donnÃ©es d'Ã©venements de feux :earth_americas:
En tant que rÃ©fÃ©rence pour Ã©tablir les datasets de training / validation / test, nous avons utilisÃ© deux ressources principales :

- Pour la partie nord amÃ©ricaine: â€œFire Perimeters in California Database CAL FIRE, provided by the Fire and Resource Assessment Program (FRAP)â€

- Pour la partie EuropÃ©enne, nous avons exploitÃ© la base de donnÃ©es des feux au Portugal de lâ€™institut INCF.

![Shapefile](https://github.com/DataScientest-Studio/firepy/blob/main/resources/image25.png?raw=true)

Pour chaque source, il Ã©tait possible de tÃ©lÃ©charger un fichier shapefile contenant lâ€™ensemble des incendies. Lâ€™outil QGIS a permis de contrÃ´ler les gÃ©omÃ©tries puis de rÃ©cupÃ©rer les gÃ©omÃ©tries de chaque zone brÃ»lÃ©e dans un fichier dÃ©diÃ©.

Ces pÃ©rimÃ¨tres vont nous aider Ã  rÃ©cupÃ©rer les images correspondant aux Ã©vÃ©nements dans des limites dâ€™emprises pertinentes, mais aussi Ã  construire le jeu d'entraÃ®nement et Ã  contrÃ´ler nos rÃ©sultats.


## :hotsprings:SÃ©lection des incendies pour la base de donnÃ©es d'entraÃ®nement :hotsprings:
La lecture des bases de donnÃ©es Shapefile de la Californie et du Portugal permet dâ€™obtenir un â€œGeo-dataframe pandasâ€ qui est une extension du dataframe Pandas avec un gÃ©o-rÃ©fÃ©rencement de chaque observation.

![geodf](https://github.com/DataScientest-Studio/firepy/blob/main/resources/image11.png?raw=true)

Pour chaque Ã©vÃ¨nement, nous avons donc une colonne avec le pÃ©rimÃ¨tre de zones brÃ»lÃ©es sous la forme dâ€™une liste de polygones dÃ©finis par des coordonnÃ©es
A lâ€™issue de la rÃ©cupÃ©ration des Shapefiles contenant plusieurs centaines dâ€™incendies, nous avons scriptÃ© lâ€™extraction des Ã©lÃ©ments suivant : longitude_1 et longitude_2, latitude_1 et latitude_2 (afin dâ€™obtenir la bounding box en coordonnÃ©es GPS), date de dÃ©but de lâ€™incendie, date de fin de lâ€™incendie (pour dater et faciliter la recherche des images Sentinel 2 prÃ©-fire et post-fire). Ces informations permettent de solliciter le service Google Earth Engine afin de tÃ©lÃ©charger les images Sentinel 2.

## :globe_with_meridians:GÃ©nÃ©ration des images Sentinel 2 :globe_with_meridians:
Google Earth Engine est un service fournissant un catalogue dâ€™images satellite et de dataset gÃ©o-rÃ©fÃ©rencÃ©s de plusieurs petabytes. Des capacitÃ©s dâ€™analyse de la surface de la Terre sont mises Ã  la disposition des chercheurs et dÃ©veloppeurs. 
Ce service gratuit est un bon moyen de collecte des gros volumes de donnÃ©es dâ€™imagerie satellitaire Sentinel 2.

Lâ€™extraction des donnÃ©es Sentinel 2 a Ã©tÃ© codÃ©e de la faÃ§on suivante:

![image](https://user-images.githubusercontent.com/31386060/155056296-37279b72-e34a-4485-a032-bb3bfce3208d.png)

RequÃªte du catalogue Sentinel 2
- Filtrage sur la zone dâ€™intÃ©rÃªt via les coordonnÃ©es GPS issues du gÃ©o-dataframe
- Filtrage sur la pÃ©riode dâ€™intÃ©rÃªt issue du gÃ©o-dataframe
- Filtrage sur les images contenant moins de 10% de couverture nuageuse
- Filtrage sur les bandes du capteur (3 canaux RGB + 2 infra-rouge NIR, SWI). Dâ€™aprÃ¨s certaines publications, le choix de ces 5 frÃ©quences produit en effet les meilleures performances de classification.
- Reconstruction et fusion dâ€™une mosaÃ¯que dâ€™images (par empilement)
- DÃ©coupage sur la zone dâ€™intÃ©rÃªt

## :face_in_clouds: GÃ©nÃ©ration des masques :face_in_clouds:
La gÃ©nÃ©ration du masque se dÃ©roule en 4 Ã©tapes:
- PrÃ©paration dâ€™une image raster de masque construite sur les caractÃ©ristiques de lâ€™image Sentinel 2 correspondante et contenant des valeurs nulles. La librairie GDAL contenant le driver â€œGTiffâ€ a Ã©tÃ© utilisÃ©e. 
- RÃ©cupÃ©ration de la gÃ©omÃ©trie issue du shapefile correspondant. La librairie OGR contenant le driver â€œESRI shapefileâ€ a Ã©tÃ© utilisÃ©e.
- Ajout de la gÃ©omÃ©trie de la zone brÃ»lÃ©e Ã  lâ€™image raster de masque.
- Export de lâ€™image raster de masque en fichier tiff.

![image2](https://github.com/DataScientest-Studio/firepy/blob/main/resources/image8.png?raw=true)


## :chart: Pre-processing des donnÃ©es :chart:
- Verification de la QualitÃ© des donnÃ©es (prÃ©sence de fumÃ©e/neige/erreur de labellisation)
- DÃ©coupage en patchs : (ğ‘›ğ‘ ğ‘ğ‘ğ‘¡ğ‘â„, ğ‘›ğ‘ ğ‘ğ‘–ğ‘¥ğ‘’ğ‘™ğ‘  â„ğ‘ğ‘¢ğ‘¡ğ‘’ğ‘¢ğ‘Ÿ, ğ‘›ğ‘ ğ‘ğ‘–ğ‘¥ğ‘’ğ‘™ğ‘  ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¢ğ‘Ÿ, ğ‘›ğ‘ ğ‘ğ‘ğ‘›ğ‘ğ‘¢ğ‘¥) soit : (32, 256, 256, 5)
- Normalisation
- Augmentation des donnÃ©es
- Dimensions du dataset dâ€™entrÃ©e

## :computer: ModÃ©lisation de type â€œsegmentation sÃ©mantiqueâ€ :computer:
## U-Net
La modÃ©lisation U-net a Ã©tÃ© proposÃ©e dans la publication â€œU-Net: Convolutional Networks for Biomedical Image Segmentationâ€. Cette mÃ©thode de classification est de type FCNN â€œFully Convolutional Neural Networkâ€, c'est-Ã -dire sans couches fully connected.

Lâ€™architecture U-Net semble pertinente pour la segmentation dâ€™images satellites. En effet, ce type de modÃ©lisation a dÃ©montrÃ© de bons rÃ©sultats pour des tÃ¢ches similaires et pour des bases de donnÃ©es d'entraÃ®nement peu fournies.

![image_unet](https://github.com/DataScientest-Studio/firepy/blob/main/resources/image12.png?raw=true)

![image](https://user-images.githubusercontent.com/31386060/155056814-e708519e-fa47-46c3-b90a-45bf4904f97e.png)
![image](https://user-images.githubusercontent.com/31386060/155056833-8730237c-7d3f-4470-a4b6-5c17b1e210e9.png)
![image](https://user-images.githubusercontent.com/31386060/155056790-e3b1591b-d617-48b2-b571-51260a77fe8e.png)

## PSP-Net
Afin de confronter le modÃ¨le U-Net, il a Ã©tÃ© proposÃ© (suite Ã  plusieurs articles et Githubs mettant en avant ce modÃ¨le dans la segmentation dâ€™images satellite) dâ€™utiliser le modÃ¨le PSPNet (Pyramid Scene Parsing Network).

![image](https://user-images.githubusercontent.com/31386060/155057002-94959f73-2123-4ada-9739-4b66fd4b02a3.png)


La difficultÃ© rÃ©side dans le fait que ce modÃ¨le peut s'avÃ©rer puissant mais uniquement sur des images constituÃ©es de 3 bandes (RGB) et devient trop gourmand en ressources sur nos fichiers en 5 bandes car lÃ  oÃ¹ le modÃ¨le U-Net nÃ©cessite 1,941,393 paramÃ¨tres, le PSP requiert 31,203,073 paramÃ¨tres.
La modÃ©lisation a Ã©tÃ© effectuÃ©e sur lâ€™ensemble des donnÃ©es en Patch, mais sâ€™est vite soldÃ©e par un Ã©chec car la RAM disponible sur les serveurs Colab nâ€™Ã©tait pas suffisante.
Afin de tout de mÃªme lui laisser ses chances, nous avons procÃ©dÃ© Ã  un redimensionnement des images afin d'allÃ©ger la charge mais les rÃ©sultats nâ€™Ã©taient pas concluants non plus (nous pouvons constater que la prÃ©diction a des artefacts et nâ€™est pas aussi prÃ©cise que pour le modÃ¨le U-Net)

![image](https://user-images.githubusercontent.com/31386060/155057035-a1190556-612e-45f7-9db0-6e152228c2d1.png)

## :mag: PrÃ©diction sur de nouvelles images :mag:

Le modÃ¨le appris sur le dataset d'entraÃ®nement peut Ãªtre appliquÃ© Ã  de nouvelles images. 

Cette tÃ¢che de prÃ©diction se dÃ©roule en 3 Ã©tapes:
1. DÃ©coupage dâ€™une image Sentinel 2 en patchs de 256 par 256 pixels
2. PrÃ©diction des zones brÃ»lÃ©es sur chaque patch Ã  lâ€™aide du modÃ¨le
3. Reconstitution dâ€™une prÃ©diction globale Ã  partir des patchs prÃ©dits

Pour la derniÃ¨re Ã©tape, 2 mÃ©thodes ont Ã©tÃ© mise en oeuvre:
Juxtaposition des patchs prÃ©dits. Le modÃ¨le est appliquÃ© sur chaque patch et les rÃ©sultats sont simplement collÃ©s en respectant lâ€™ordre du dÃ©coupage. Cette technique est disponible via la mÃ©thode unpatchify du package patchify. Bien que rapide, cette mÃ©thode fait apparaÃ®tre des artÃ©facts sur les bords de chaque bord. Le modÃ¨le a en effet plus de difficultÃ©s Ã  apprendre prÃ¨s des bords. De plus, les prÃ©dictions ne sont possibles que sur un nombre fini de patchs, ce qui peut exclure une partie de lâ€™image. Voir image en bas Ã  gauche.
â€œSmoothed blendingâ€ des patchs prÃ©dits. Dans cet algorithme, le modÃ¨le est appliquÃ© sur une fenÃªtre glissante de lâ€™image satellite avec un overlap entre chaque tuile. Ensuite, les rÃ©sultats de prÃ©diction sont recombinÃ©s ensemble avec une interpolation de type spline. Lâ€™algorithme utilisÃ© est issu dâ€™un projet open source (https://github.com/Vooban/Smoothly-Blend-Image-Patches). Bien que nÃ©cessitant plus de temps de calcul, cette mÃ©thode est plus performante et les images produites sont trÃ¨s rÃ©alistes. Voir image en bas Ã  droite.

![image](https://github.com/DataScientest-Studio/firepy/blob/f49526d30c78fbaa1f4356544c7e5d84d3810c2e/resources/U_Net%20Prediction%20result%20on%20California%20imagery.png)

On remarque que le masque de zone brÃ»lÃ©e issu de la prÃ©diction est trÃ¨s proche de la vÃ©ritÃ© terrain. Lâ€™affichage de la probabilitÃ© de zone brÃ»lÃ©e permet de faire apparaÃ®tre des nuances dans le niveau de brÃ»lure ainsi que des Ã©lÃ©ments internes (cours dâ€™eau, routesâ€¦). Cette nouvelle connaissance est intÃ©ressante pour Ã©valuer l'impact du feu sur les points stratÃ©giques de la zone Ã©tudiÃ©e. Une meilleure gestion des consÃ©quences de lâ€™incendie est alors possible.





