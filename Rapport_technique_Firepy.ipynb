% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\begin{document}


\begin{titlepage}
\title{Rapport Technique d'évaluation\\
FirePy}
\date{Promotion : Data Scientist mars 2022}
\author{Participants : Emmanuelle Cano, François Faupin, Thomas Gossart
\and Mentor : Pierre Adeikalam}
\maketitle
\begin{center}
{\includegraphics{images/image4.png}}
{\includegraphics{images/image12.png}}
\end{center}

\end{titlepage}


    \tableofcontents
\newpage


\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

{}

\hypertarget{h.qvkqsisw6tbi}{%
\section{\texorpdfstring{{Contexte}}{Contexte}}\label{h.qvkqsisw6tbi}}

{}

\hypertarget{h.t2ik57n0sgfa}{%
\subsection{\texorpdfstring{{Dans l'actualité
}}{Dans l'actualité }}\label{h.t2ik57n0sgfa}}

{Les incendies de forêt augmentent en intensité et en fréquence à
travers le monde en raison du changement climatique et de la hausse de
la température mondiale. }

{}

{L'observation de la Terre est un atout permettant de mieux comprendre
et mesurer l'impact pour les populations et les infrastructures.}

{Les techniques de data science appliquées aux données satellitaires
semblent pertinentes pour cet enjeu de surveillance de la surface
terrestre. }

{L'enjeu est de pouvoir être capable de détecter des zones brûlées sur
n'importe quelle partie du globe, surveiller la progression des
incendies de forêt en temps quasi réel, ce qui est donc d'une importance
cruciale pour les interventions d'urgence, mais aussi pour une
estimation des enjeux économiques.}

{}

{}

\hypertarget{h.hiicrszhglmr}{%
\subsection{\texorpdfstring{{Dans la formation
}}{Dans la formation }}\label{h.hiicrszhglmr}}

{Emmanuelle, de formation géomatique et télédétection, sans qui ce
projet n'aurait pas vu le jour, à mis en œuvre la récupération des
données de validation via QGIS,}

{François, Data Analyst, de par son expertise, avait déjà les
connaissances théoriques et pratique en Deep Learning}

{Thomas, novice en Data Science au début du projet.}

{}

{Ce projet était un puissant levier de montée en compétences sur les
diverses problématiques et solutions en Data Science. }

{}

{}

{}
\newpage
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

{}

\hypertarget{h.t91thr827huz}{%
\section{\texorpdfstring{{Objectifs du
projet}}{Objectifs du projet}}\label{h.t91thr827huz}}

{}

\hypertarget{h.su63rliu68rc}{%
\subsection{\texorpdfstring{{Business
case}}{Business case}}\label{h.su63rliu68rc}}

{}

{Le relevé manuel des périmètres de feu est une tâche fastidieuse et
sujette à erreur humaine. Le niveau de détails obtenu est limité car des
poches épargnées par les flammes peuvent se trouver à l'intérieur des
zones de feu.}

{}

{L'automatisation de la détection de surfaces brûlées par un algorithme
permettrait d'alléger la charge de travail humaine, d'apporter de la
robustesse dans l'analyse, de passer à l'échelle la zone d'analyse et
permettrait d'apporter une estimation préliminaire quant aux dégâts
(naturels, les biens, les infrastructures). Le traitement mathématique
des images satellites est aussi un moyen d'aller au-delà d'une
information binaire brûlé / non brûlé et d'affiner le niveau de
brûlure.}

{}

{Les méthodes traditionnelles de détection de zones brûlées ont des
performances limitées. En fonctionnant à l'aide de seuils, il est
difficile pour ces techniques de détecter des petites zones brûlées ou
des brûlures de faible intensité. Certains algorithmes basés sur la
détection d'anomalie sur le voisinage de pixels ont un taux élevé de
fausse détection. Enfin, les méthodes exploitant les différences dans
les séquences d'image dans le temps ont l'inconvénient de nécessiter
beaucoup de données.}

{Face à ce constat, les algorithmes de deep learning semblent
particulièrement prometteurs et font l'objet de nombreuses recherches et
publications.}

{}

{La bonne disponibilité des données des satellites Sentinel 2 financés
par le programme Copernicus de l'ESA est un atout essentiel pour le
lancement du projet. En effet, les images sont disponibles gratuitement
et couvrent depuis 2015 une grande partie de la surface terrestre.}

{}

{}

{En résumé:}

{}

{L'objectif du projet est de mettre au point un algorithme capable de
détecter les zones brûlées suite à un incendie à partir des images des
satellites Sentinel 2 et avec une résolution très précise (au niveau du
pixel).}

{}

{}

{}
\newpage

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}


\subsection{\texorpdfstring{{Méthodologie et environnement de travail}}{Méthodologie et environnement de travail}}\label{h.j5af2y3qv1bw}

{

{Dans la cartographie des techniques de machine learning, le challenge
du projet correspond à une tâche de classification supervisée et, plus
particulièrement, de segmentation sémantique. Il s'agit de classifier
chaque pixel d'une image (pixel brûlé / non brûlé).}

{}

{Afin d'atteindre }{cet }{objectif, la méthodologie globale appliquée
suivra les étapes ci-dessous:}

\begin{itemize}
\tightlist
\item
  {Recherche bibliographique sur les méthodes de détection de zones
  brûlées}
\item
  {Constitution d'une base de données suffisamment propre et variée}
\end{itemize}

\begin{itemize}
\tightlist
\item
  {Entrainement d'un algorithme de Deep Learning à partir d'une région
  pour laquelle des données de vérité terrain (informations collectées
  par l'homme et non automatisées) sont disponibles,}
\item
  {Application de l'algorithme à d'autres scènes et d'autres incendies
  pour produire une cartographie du contour de la zone brûlée une fois
  le feu maîtrisé par classification au pixel. }
\end{itemize}

{}

{La nature des données d'entrée a nécessité une m}{ontée en compétences
sur les images géo-référencées. En effet, des structures de données
particulières (Fichiers vecteur Shapefile, Raster, GeoDataframe\ldots)
et des outils dédiés (QGIS, Geopandas, Rasterio\footnote{\href{https://rasterio.readthedocs.io/en/latest/}{https://rasterio.readthedocs.io/en/latest/}}) ont dû être mis en œuvre.}

{}

{Enfin, afin de pouvoir travailler en collaboratif sur un environnement
permettant les calculs de deep learning, nous avons utilisé les outils
Google (Drive, Colab, Google Earth Engine) ainsi qu'un espace Github
dédié.}
\newpage
{}

{}

{}

{}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

{}

\hypertarget{h.5dqnue7b735q}{%
\section{\texorpdfstring{{Déroulement du
projet}}{Déroulement du projet}}\label{h.5dqnue7b735q}}

\hypertarget{h.c4k7xi82glh7}{%
\subsection{\texorpdfstring{{Collecte des données}}{Collecte des données}}\label{h.c4k7xi82glh7}}

{Pour réaliser ce projet, nous avons eu besoin de deux types distincts de données.\\
Les images satellite, qui serviront pour l'entraînement du modèle, et les labels, pour la vérification.}

{}

\hypertarget{h.ejnampsx37ol}{%
\subsubsection{\texorpdfstring{{Description des données des satellites
Sentinel 2
}}{Description des données des satellites Sentinel 2 }}\label{h.ejnampsx37ol}}

{}

{Les 2 satellites {Sentinel 2}\footnote{\href{https://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-2-msi/resolutions/spatial}{https://sentinels.copernicus.eu/.../resolutions/spatial}} (2A et 2B) ont été déployés en juin 2015 dans le cadre du programme
Copernicus financé par l'Union Européenne et géré par {l'ESA}\footnote{\href{https://sentinel.esa.int/web/sentinel/missions/sentinel-2}{https://sentinel.esa.int/web/sentinel/missions/sentinel-2}}}{(European Spatial
Agency). L'objectif est de mettre à disposition des informations sur le sol, les océans, l'atmosphère et la sécurité.}

{}

{\includegraphics{images/image23.jpg}}

{}

{Le capteur multi-spectral de Sentinel 2 (MSI) permet de réaliser des acquisitions dans 13 bandes
spectrales de résolutions spatiales différentes (de 10 à 60 m) dans les domaines du visible, du proche et du moyen infrarouge.}

{\includegraphics{images/image31.png}}


{L'indice permettant de détecter les zones brûlées fera appel aux bandes
7 (NIR) et 12 (SWIR), toutes les deux à 20 m de résolution spatiale.}

{On pourra également utiliser les bandes du visible (2,3,4) pour
réaliser des compositions colorées permettant d'analyser visuellement
les territoires à traiter ainsi que de calculer d'autres indices en lien
avec la densité de végétation.}

{}

{En effet, les surfaces brûlées présentent une réflectance plus faible
que la végétation saine dans le moyen infrarouge, en raison de
l'absorption des radiations par les cendres, ceci indépendamment des
écosystèmes.}

{Toutefois, cette réflectance des surfaces brûlées dans le moyen
infrarouge est voisine de celles des surfaces très humides et rend les
confusions possibles. }{Ceci justifie la prise en compte d'un deuxième
discriminant plus sévère, l'augmentation des températures de surface
dans les zones brûlées pendant la journée en raison de la forte
absorption des radiations solaires et de l'absence de
l'évapotranspiration qui, dans les conditions normales, assure le
transfert de l'énergie dans l'atmosphère sous forme de chaleur latente,
à travers la vapeur d'eau.}{~De plus, la présence des cendres et du
charbon décroît l'albédo\footnote{Grandeur caractérisant la proportion d’énergie lumineuse réfléchie ou diffusée par un corps éclairé.} de surface et en augmente la température d'environ 7 à 8° Kelvin.}

{}

\hypertarget{h.m69o7xe9kx61}{%
\subsubsection{\texorpdfstring{{Description des données d'événements de
feux}}{Description des données d'événements de feux}}\label{h.m69o7xe9kx61}}

{}

{En tant que référence pour établir les datasets de training /
validation / test, nous avons utilisé deux ressources principales :}

{}

\begin{itemize}
\tightlist
\item
Pour la partie nord américaine:  ``{Fire Perimeters in California  Database}\footnote{\href{ https://www.fire.ca.gov/incidents}{ https://www.fire.ca.gov/incidents}}(CALFIRE), provided by the
  {Fire and Resource Assessment  Program}\footnote{\href{https://frap.fire.ca.gov/frap-projects/fire-perimeters/}{https://frap.fire.ca.gov/frap-projects/fire-perimeters/}} (FRAP)''
\end{itemize}

{}

{Il est ainsi possible de récupérer la base de données du projet FRAP du
gouvernement californien, qui met notamment à disposition, en complément
des caractéristiques historiques des incendies survenus dans le pays
depuis 1950, les périmètres des zones brûlées sous forme de données
géographiques vecteurs (géodatabase ESRI). }

{}

{\includegraphics{images/image22.png}}
\begin{flushright}
\it
{Shapefile contenant l'ensemble des incendies de la base CALFIRE}
\end{flushright}
{}

{Pour produire le jeu de données d'entraînement, le choix s'est
principalement porté sur les incendies de la Californie entre 2018 et
2020. Ces événements sont en effet richement documentés (voir ci-dessous
des exemples d'événements récents).}

{\includegraphics{images/image7.png}}

{}

\begin{itemize}
\tightlist
\item
  Pour la partie Européenne, nous avons exploité  la base de données
  des feux au Portugal de l'institut INCF\footnote{\href{http://www2.icnf.pt/portal/florestas/dfci/inc/cartografia/areas-ardidas}{http://www2.icnf.pt/portal/florestas/dfci/inc/cartografia/areas-ardidas}} (Instituto da
  Conservação da Natureza e das Florestas).
\end{itemize}

{}

\begin{center}
{\includegraphics{images/image29.png}}
\end{center}

{}

{Pour chaque source, il était possible de télécharger un fichier
shapefile contenant l'ensemble des incendies. L'outil QGIS\footnote{\href{https://www.qgis.org/fr/site/}{https://www.qgis.org/fr/site/}} a permis de contrôler les
géométries puis de récupérer les géométries de chaque zone brûlée dans
un fichier dédié.}

{}

{Tout d'abord, qu'est ce qu'un shapefile ?}

{Un shapefile est un format de fichier pour les systèmes d'informations
géographiques (SIG) et contient toute l'information liée à la géométrie
des objets décrits, qui peuvent être des points, des lignes ou des
polygones.}

{}

{D'après la description de la méthodologie sur ces sites, ces périmètres
ne sont pas produits à partir de méthodes automatisées mais de
digitalisation manuelle à partir de cartes ou de photointerprétation. La
majorité des périmètres de feu est réalisée au travers de relevés GPS au
sol.}

{}

{\includegraphics{images/image21.png}}{~}

{}

{Ces périmètres vont nous aider à récupérer les images correspondant aux
événements dans des limites d'emprises pertinentes, mais aussi à
construire le jeu d'entraînement et à contrôler nos résultats.}

{}

\hypertarget{h.nfpzgdjy3er7}{%
\subsubsection{\texorpdfstring{{Sélection des incendies pour la base de
données
d'entraînement}}{Sélection des incendies pour la base de données d'entraînement}}\label{h.nfpzgdjy3er7}}

{}

{La lecture des bases de données Shapefile de la Californie et du
Portugal permet d'obtenir un ``Geo-dataframe pandas'' qui est une
extension du dataframe Pandas avec un géo-référencement de chaque
observation.}

{Pour chaque évènement, nous avons donc une colonne avec le périmètre de
zones brûlées sous la forme d'une liste de polygones définis par des
coordonnées.}

{~}

{\includegraphics{images/image9.png}}

{}

{Dans notre démarche, nous avons privilégié les gros incendies (filtrage
sur la colonne ``fire\_area'' \textgreater{} 2500 ha) afin de disposer
}{d'images d'au moins 256 x 256 pixels (1 pixel équivaut à 20m).}

{}

{A l'issue de la récupération des Shapefiles contenant plusieurs
centaines d'incendies, nous avons scripté l'extraction des éléments
suivant : longitude\_1 et longitude\_2, latitude\_1 et latitude\_2 (afin
d'obtenir la bounding box en coordonnées GPS), date de début de
l'incendie, date de fin de l'incendie (pour dater et faciliter la
recherche des images Sentinel 2 pré-fire et post-fire). Ces informations
permettent de solliciter le service Google Earth Engine afin de
télécharger les images Sentinel 2.}

{}

\hypertarget{h.nj9d7kjqxuz}{%
\subsubsection{\texorpdfstring{{Génération des images Sentinel
2}}{Génération des images Sentinel 2}}\label{h.nj9d7kjqxuz}}



{Google Earth Engine}\footnote{\href{https://earthengine.google.com/}{https://earthengine.google.com/}} est un
service fournissant un catalogue d'images satellite et de dataset
géo-référencés de plusieurs petabytes. Des capacités d'analyse de la
surface de la Terre sont mises à la disposition des chercheurs et
développeurs.

{Ce service gratuit est un bon moyen de collecte des gros volumes de
données d'imagerie satellitaire Sentinel 2.}

{}

{L'extraction des données Sentinel 2 a été codée de la façon suivante:}

{\includegraphics{images/image20.png}}

{}

\begin{itemize}
\tightlist
\item
  {Requête du catalogue Sentinel 2}
\item
  {Filtrage sur la zone d'intérêt via les coordonnées GPS issues du
  géo-dataframe}
\item
  {Filtrage sur la période d'intérêt issue du géo-dataframe}
\item
  {Filtrage sur les images contenant moins de 10\% de couverture
  nuageuse}
\item
  {Filtrage sur les bandes du capteur (3 canaux RGB + 2 infra-rouge NIR,
  SWI). D'après certaines publications, le choix de ces 5 fréquences
  produit en effet les meilleures performances de classification.}
\item
  {Reconstruction et fusion d'une mosaïque d'images (par empilement)}
\item
  {Découpage sur la zone d'intérêt}
\end{itemize}

{}

{Une fois l'objet image généré, il est possible de le télécharger avec
la résolution souhaitée (20m) dans un fichier qui sera stocké dans le
drive Google. Nous avons décidé que le fichier de sortie serait un
fichier raster TIF car lui-seul nous permettait d'avoir à la fois les 5
canaux, aucune perte de qualité sur les fichiers volumineux, ainsi que
les métadonnées associées.}

{}
\newpage

{Qu'est-ce qu'une image raster?}

{}

{Ce format est utilisé pour exploiter les images satellites, aériennes
ou de plan. Cette représentation d'une photographie ou d'un plan est
stockée sous la forme d'une grille de pixels en ligne et en colonne.
Chaque cellule de cette matrice contient à la fois l'intensité des
canaux et les coordonnées géographiques.}

{}

{\includegraphics{images/image16.png}}

{}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}


\subsubsection{\texorpdfstring{{Génération des
masques}}{Génération des masques}}\label{h.vyjgf56vbjyj}}

{}

{La génération du masque se déroule en 4 étapes:}

\begin{itemize}
\tightlist
\item
  {Préparation d'une image raster de masque construite sur les
  caractéristiques de l'image Sentinel 2 correspondante et contenant des
  valeurs nulles. La librairie
  GDAL\footnote{\href{https://gdal.org/}{https://gdal.org/}} contenant le driver ``GTiff'' a été utilisée. }
\item
  {Récupération de la géométrie issue du shapefile correspondant. La
  librairie
 OGR\footnote{\href{https://gdal.org/}{https://gdal.org/}} contenant le driver ``ESRI shapefile'' a été utilisée.}
\item
  {Ajout de la géométrie de la zone brûlée à l'image raster de masque.}
\item
  {Export de l'image raster de masque en fichier tiff.}
\end{itemize}

{}

{Voici un exemple de masque et la vue en couleurs naturelles de l'image
satellite associée:}

{}

{\includegraphics{images/image6.png}}

{}

{}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}


\hypertarget{h.lwlcxoemf3tj}{%
\subsection{\texorpdfstring{{Pre-processing des
données}}{Pre-processing des données}}\label{h.lwlcxoemf3tj}}

{}

\hypertarget{h.1scggrj750q}{%
\subsubsection{\texorpdfstring{{Qualité des
données}}{Qualité des données}}\label{h.1scggrj750q}}

{}

{La visualisation des images en couleurs naturelles RGB a mis en
évidence des problèmes de qualité de données. Certaines images doivent
être écartées du dataset d'entraînement ~afin de ne pas dégrader les
performances de classification. Malgré le faible nombre d'images à
disposition, il a été décidé de privilégier la qualité des données.}

{}

{Les facteurs de non qualité sont liés à la présence d'éléments entre la
zone brûlée et le capteur du satellite. Contrairement à d'autres
satellites disposant d'un capteur actif (radar envoyant une onde
électromagnétique traversant les éléments), les images Sentinel 2 ne
font que recevoir les émissions de la Terre dans différentes fréquences.
Tout élément non désiré entre le capteur et la zone brûlée est donc
perturbateur pour l'entraînement du modèle.}

{}

{Les phénomènes conduisant à exclure certaines images sont illustrés
ci-dessous : }

\newpage
{}

\begin{itemize}
\tightlist
\item
  {Présence de fumées}
\end{itemize}

{}

{Pour certains événements, les dates de départ et de fin de feu sont
peut-être erronées de quelques jours. Nous obtenons ainsi quelques
images avec la présence massive de fumée qui semble donner un effet de
flou.}

{}

{\includegraphics{images/image26.png}}

{}

{}

{}

{}

{}

{}
\newpage
{}

\begin{itemize}
\tightlist
\item
  {Présence de neige}
\end{itemize}

{}

{La plupart des feux sont situés dans des zones montagneuses. Il est
donc possible de visualiser de la neige sur les reliefs et cela masque
les zones brûlées.}

{}

{\includegraphics{images/image14.png}}

{}

{}

{}

{}

{Enfin, il faut noter une limitation au niveau de la qualité des données
de labellisation issues des relevés topographiques. En zoomant sur
certaines images, certaines zones ont été déclarées ``non brûlées" alors
qu'on peut penser qu'il s'agit d'une erreur humaine.}

{Ceci est un point de vigilance sur les métriques de performance du
modèle de classification qui ne pourra pas atteindre 100\% à juste
titre.}

{}


{}
\newpage
{Vous trouverez ci-dessous des exemples d'erreurs de labellisation.}

{}

{\includegraphics{images/image24.png}}

{}

{}

{}
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{h.51rfm31yri3o}{%
\subsubsection{\texorpdfstring{{Découpage en
patchs}}{Découpage en patchs}}\label{h.51rfm31yri3o}}

{}

{La taille des images dépend de l'étendue des feux. Il n'est pas
conseillé d'effectuer un ``resize'' de chaque image car le modèle risque
d'apprendre des interpolations d'image très différentes. Ainsi, la bonne
pratique est de découper des patchs de 256 par 256 pixels que l'on
pourra soumettre à l'algorithme de segmentation sémantique.}

{}

{L'extraction de patchs a été réalisée via le package
``Patchify\footnote{\href{https://pypi.org/project/patchify/}{https://pypi.org/project/patchify/}}''.}

{L'exploitation des patchs permet de démultiplier le nombre d'images
pour l'apprentissage. Ainsi, une trentaine de feux génère environ 1200
patchs de 256 par 256 pixels. }

{}

{Ce même package est capable de reconstituer une grande image initiale à
partir de patchs. Cette fonctionnalité (``unpatchify'') est très utile
pour réaliser des prédictions sur des grandes images}

\hypertarget{h.btulwa26669t}{%
\subsubsection{\texorpdfstring{{Normalisation}}{Normalisation}}\label{h.btulwa26669t}}

{}

{Les données ont été normalisées afin de contenir que des valeurs
comprises entre 0 et 1.}

{}

{Les facteurs multiplicatifs suivants ont été appliqués:}

\begin{itemize}
\tightlist
\item
  {1 / 10000 pour les images Sentinel (la réflectance mesurée par le
  capteur du satellite est en effet multipliée par 10000 pour des
  raisons de stockage en entier)}
\item
  {1/ 255 pour les masques de label}
\end{itemize}

{}

{Ce traitement a été intégré au sein du générateur fournissant au modèle
les données à la volée.}

\hypertarget{h.jt1w7v35u5vc}{%
\subsubsection{\texorpdfstring{{Augmentation des
données}}{Augmentation des données}}\label{h.jt1w7v35u5vc}}

{}

{Afin d'augmenter le nombre d'images pour l'apprentissage, une stratégie
d'augmentation de données a été mise en place. }

{Le package ``Albumentations\footnote{\href{https://albumentations.ai/}{https://albumentations.ai/}}''
a été sélectionné pour cette tâche car il permet de gérer les images
possédant 5 canaux. }

{}

{Ce traitement a été intégré au sein du générateur fournissant au modèle
les données à la volée.}

{Les augmentations d'image implémentées sont les suivantes:}

\begin{itemize}
\tightlist
\item
  {Probabilité de 30\% de flip horizontal}
\item
  {Probabilité de 30\% de flip vertical}
\item
  {Probabilité de 30\% de rotation de 90°}
\end{itemize}

\hypertarget{h.eay2cv105yx1}{%
\subsubsection{\texorpdfstring{{Dimensions du dataset
d'entrée}}{Dimensions du dataset d'entrée}}\label{h.eay2cv105yx1}}

{}

{A l'issue des différents traitements de pré-processing, le dataset se
présente sous la forme d'un batch d'images à 5 canaux.}

{}

{Le nettoyage des images Sentinel a conduit à la constitution d'une base
de données de 1200 patchs issus des feux de Californie et Portugal. Ces
images ne tiennent pas toutes en mémoire RAM. Elles sont donc fournies
au modèle via des batchs mis à disposition par un générateur
personnalisé.}

{}
\newpage
{Les dimensions du dataset en entrée du modèle sont:}

{}

{Pour une image Sentinel 2:}

{}
    
\begin{center}
\includegraphics{images/image1.png}

\includegraphics{images/image2.png}
\end{center}
{}

{Pour un masque labellisant les zones brûlées:}

{}
\begin{center}
\includegraphics{images/image1.png}

\includegraphics{images/image3.png}
\end{center}

{}

{In fine, notre base de données est constituée d'environ 1200 patchs
avec le masque associé.}

{}

{Voici un exemple ci-dessous:}

{}

{\includegraphics{images/image15.png}}
\newpage
\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}


\hypertarget{h.nb99w18njl6c}{%
\subsection{\texorpdfstring{{Modélisation de type ``segmentation
sémantique''}}{Modélisation de type ``segmentation sémantique''}}\label{h.nb99w18njl6c}}

\hypertarget{h.y9f77otg2lt6}{%
\subsubsection{\texorpdfstring{{U-Net}}{U-Net}}\label{h.y9f77otg2lt6}}

{}

{La modélisation U-Net\footnote{\href{https://arxiv.org/pdf/1505.04597.pdf}{Convolutional Networks for Biomedical Image Segmentation}} a été
proposée dans la publication ``U-Net: Convolutional Networks for
Biomedical Image Segmentation''. Cette méthode de classification est de
type FCNN ``Fully Convolutional Neural Network'', c'est-à-dire sans
couches fully connected.}

{}

{L'architecture U-Net semble pertinente pour la segmentation d'images
satellites. En effet, ce type de modélisation a démontré de bons
résultats pour des tâches similaires et pour des bases de données
d'entraînement peu fournies.}

{}

{}

{La structure originelle du U-Net est illustrée ci-dessous.}

{\includegraphics{images/image10.png}}

{}

{Cette architecture comprend une partie d'encodage à l'aide une séquence
de couches de convolution, de pooling et de dropout. A chaque étape de
convolution de la phase d'encodage, l'information spatiale perdue est
récupérée (flèches grises) pour être exploitée lors de la phase de
décodage.}

{La reconstruction de l'image prédite contenant la zone d'intérêt
s'effectue au travers de couches de dé-convolutions appliquées aux
features encodées et à l'information spatiale stockée pendant
l'encodage.}

{}

{Afin d'obtenir une probabilité de classification binaire dans l'image
de sortie, une fonction d'activation de type sigmoïde a été utilisée.}

{}

{}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

{}

{Le modèle codé dans le cadre du projet est illustré ci-dessous:}

{\includegraphics{images/image18.png}}

{}

{A}{près plusieurs essais, la fonction de perte la plus performante
correspond à un mix (50/50) entre la fonction de binary cross entropy et
du coefficient DICE qui correspond à une version exploitable car
différentiable de l'IOU (Intersection Over Union). Le choix de cette
fonction coût produit les meilleurs résultats de prédiction sur
l'échantillon de validation (voir tableau ci-dessous).}

{}

{\includegraphics{images/image27.png}}

{}

{La compilation du modèle a été réalisée avec l'optimizer Adam (et son
paramétrage par défaut) et les métriques: ``Accuracy'', ``F1 score'',
``Precision'', ``Recall'' et ``IOU''.}

{}

{L'entraînement du modèle U-Net a été réalisé sur 20 epochs avec le
callback ``early stopping'' afin de stopper l'entraînement lorsque la
fonction coût augmente et de restaurer les meilleurs poids. }

{Lors de la phase d'apprentissage, un échantillon de validation (20\% du
dataset) est mis de côté dans le but de contrôler la performance de
prédiction du modèle.}

{}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

{}

{Nous obtenons les résultats suivants:}

{}

{\includegraphics{images/image5.png}}

{}

{Les métriques de performance du modèle atteignent un plateau
correspondant à une accuracy sur les données de validation autour de
91,5\% et un f1 score de 88\%. }

{L'indicateur IOU est de 74\%, ce qui est satisfaisant (car supérieur à
50\%).}

{}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

{}

{L'architecture U-Net affiche de bonnes performances de prédiction.
Avant d'exploiter cet algorithme en production, les tuiles de test mises
de côté peuvent être analysées afin de contrôler visuellement la
pertinence du modèle.}

{}

{\includegraphics{images/image30.png}}

{\includegraphics{images/image8.png}}

{\includegraphics{images/image33.png}}

{\includegraphics{images/image11.png}}

{}

{Dans l'image ci-dessous, on remarque que le lac est visible sur la
cartographie de probabilité de zone brûlée mais pas sur la prédiction
brûlé/non brûlé. Un réglage du seuil de décision semble nécessaire.}

{}

{\includegraphics{images/image28.png}}

{L'application du modèle sur les patchs ci-dessous confirme la bonne
performance de l'algorithme sur des régions non brûlées (champs
agricoles, zones urbaines).}

{}

{\includegraphics{images/image13.png}}

{}

{\includegraphics{images/image17.png}}

{}

\hypertarget{h.5lerhemqka88}{%
\subsubsection{\texorpdfstring{{PSP-Net}}{PSP-Net}}\label{h.5lerhemqka88}}

{Afin de confronter le modèle U-Net, il a été proposé (suite à plusieurs
articles et Githubs mettant en avant ce modèle dans la segmentation
d'images satellite) d'utiliser le modèle
PSPNet\footnote{\href{https://arxiv.org/abs/1612.01105}{https://arxiv.org/abs/1612.01105}} (Pyramid Scene Parsing
Network).}

{}

{\includegraphics{images/image32.png}}

{La difficulté réside dans le fait que ce modèle peut s'avérer puissant
mais uniquement sur des images constituées de 3 bandes (RGB) et devient
trop gourmand en ressources sur nos fichiers en 5 bandes car là où le
modèle U-Net nécessite 1,941,393 paramètres, le PSP requiert 31,203,073
paramètres.\\
}

{La modélisation a été effectuée sur l'ensemble des données en Patch,
mais s'est vite soldée par un échec car la RAM disponible sur les
serveurs Colab n'était pas suffisante.}

{Afin de tout de même lui laisser ses chances, nous avons procédé à un
redimensionnement des images afin d'alléger la charge mais les résultats
n'étaient pas concluants non plus (nous pouvons constater que la
prédiction a des artefacts et n'est pas aussi précise que pour le modèle
U-Net)}

{}

{}

{\includegraphics{images/image25.png}}

{}

\hypertarget{h.7wxdx4v701iq}{%
\subsection{\texorpdfstring{{Prédiction sur de nouvelles
images}}{Prédiction sur de nouvelles images}}\label{h.7wxdx4v701iq}}

{}

{Le modèle appris sur le dataset d'entraînement peut être appliqué à de
nouvelles images. }

{}

{Cette tâche de prédiction se déroule en 3 étapes:}

\begin{enumerate}
\tightlist
\item
  {Découpage d'une image Sentinel 2 en patchs de 256 par 256 pixels}
\item
  {Prédiction des zones brûlées sur chaque patch à l'aide du modèle}
\item
  {Reconstitution d'une prédiction globale à partir des patchs prédits}
\end{enumerate}

{}

{Pour la dernière étape, 2 méthodes ont été mise en oeuvre:}

\begin{enumerate}
\tightlist
\item
  {Juxtaposition des patchs prédits. }{Le modèle est appliqué sur chaque
  patch et les résultats sont simplement collés en respectant l'ordre du
  découpage. Cette technique est disponible via la méthode unpatchify du
  package patchify. Bien que rapide, cette méthode fait apparaître des
  artéfacts sur les bords de chaque bord. Le modèle a en effet plus de
  difficultés à apprendre près des bords. De plus, les prédictions ne
  sont possibles que sur un nombre fini de patchs, ce qui peut exclure
  une partie de l'image. Voir image en bas à gauche.}
\item
  {``Smoothed blending'' des patchs prédits.}{~Dans cet algorithme, le
  modèle est appliqué sur une fenêtre glissante de l'image satellite
  avec un overlap entre chaque tuile. Ensuite, les résultats de
  prédiction sont recombinés ensemble avec une interpolation de type
  spline. L'algorithme utilisé est issu d'un projet open source {"Smoothly-Blend-Image-Patches"}\footnote{\href{https://github.com/Vooban/Smoothly-Blend-Image-Patches}{https://github.com/Vooban/Smoothly-Blend-Image-Patches}}.
  Bien que nécessitant plus de temps de calcul, cette méthode est plus
  performante et les images produites sont très réalistes. Voir image en
  bas à droite.}
\end{enumerate}

{}

{\includegraphics{images/image19.png}}

{}

{On remarque que le masque de zone brûlée issu de la prédiction est très
proche de la vérité terrain. L'affichage de la probabilité de zone
brûlée permet de faire apparaître des nuances dans le niveau de brûlure
ainsi que des éléments internes (cours d'eau, routes\ldots). Cette
nouvelle connaissance est intéressante pour évaluer l'impact du feu sur
les points stratégiques de la zone étudiée. Une meilleure gestion des
conséquences de l'incendie est alors possible.}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

{}

\hypertarget{h.ku9fv8b7qfdu}{%
\section{\texorpdfstring{{Bilan}}{Bilan}}\label{h.ku9fv8b7qfdu}}

{}

{La réalisation du projet a permis de mettre en pratique les compétences
acquises lors de la formation dans le cadre d'un challenge
particulièrement stimulant. Le développement de cette application de
machine learning a été l'opportunité de se familiariser avec un nouveau
type de données: les images géo-référencées.}

{}

{Le code du projet peut être consulté sur le Github suivant:}

{\href{https://www.google.com/url?q=https://github.com/DataScientest-Studio/firepy\&sa=D\&source=editors\&ust=1645818404104550\&usg=AOvVaw0gJUGLJZKPd5rNDAsDzyDe}{https://github.com/DataScientest-Studio/firepy}}{~}

{}

{L'application mise au point offre des perspectives d'applications
concrètes prometteuses (surveillance de la surface brûlée, impact des
incendies sur les infrastructures\ldots).}

{Cependant, avant d'être mis en production, le modèle de détection de
zones brûlées nécessiterait d'être entraîné sur davantage de données.
L'entraînement a principalement été réalisé sur des images de
Californie, donc la généralisation aux feux d'autres régions du monde
serait limitée. En effet, la diversité des environnements (forêt, type
de végétation, volcan) n'est pas assez représentée dans le training
dataset.}

{}

{}

{}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

{}

\hypertarget{h.6l4y0m2xtrwv}{%
\section{\texorpdfstring{{Bibliographie}}{Bibliographie}}\label{h.6l4y0m2xtrwv}}

{}

{}

\protect\hypertarget{id.occ3q2s40ssb}{}{}

\begin{enumerate}
\tightlist
\item
  {U-Net: Convolutional Networks for Biomedical Image Segmentation
  }{\href{https://www.google.com/url?q=https://arxiv.org/pdf/1505.04597.pdf\&sa=D\&source=editors\&ust=1645818404105777\&usg=AOvVaw2iYop70GQdI80l52EJttQj}{https://arxiv.org/pdf/1505.04597.pdf}}{~}
\end{enumerate}

\protect\hypertarget{id.3irb46con6wm}{}{}

\begin{enumerate}
\setcounter{enumi}{1}
\tightlist
\item
  {CAL FIRE Incidents
  }{\href{https://www.google.com/url?q=https://www.fire.ca.gov/incidents\&sa=D\&source=editors\&ust=1645818404106226\&usg=AOvVaw29TO6YnrtlUQdDhV7Da82J}{https://www.fire.ca.gov/incidents}}{~}
\end{enumerate}

\protect\hypertarget{id.6ukv5glj29qq}{}{}

\begin{enumerate}
\setcounter{enumi}{2}
\tightlist
\item
  {Portugal Cartographia de area ardida
  }{\href{https://www.google.com/url?q=http://www2.icnf.pt/portal/florestas/dfci/inc/cartografia/areas-ardidas\&sa=D\&source=editors\&ust=1645818404106684\&usg=AOvVaw23R2TOcEZ_pM_SBcC1ONyN}{http://www2.icnf.pt/portal/florestas/dfci/inc/cartografia/areas-ardidas}}{~
  }
\end{enumerate}

\protect\hypertarget{id.62icuvnkoom}{}{}

\begin{enumerate}
\setcounter{enumi}{3}
\tightlist
\item
  {FRAP
  }{\href{https://www.google.com/url?q=https://frap.fire.ca.gov/frap-projects/fire-perimeters/\&sa=D\&source=editors\&ust=1645818404107111\&usg=AOvVaw2P1ZL1vACR_lFs14nGlJ-7}{https://frap.fire.ca.gov/frap-projects/fire-perimeters/}}
\end{enumerate}

\protect\hypertarget{id.uuswh49r4yyh}{}{}

\begin{enumerate}
\setcounter{enumi}{4}
\tightlist
\item
  {USA :
  }{\href{https://www.google.com/url?q=https://data-nifc.opendata.arcgis.com/\&sa=D\&source=editors\&ust=1645818404107520\&usg=AOvVaw18oihU7v3qM3KATEKOMXn9}{https://data-nifc.opendata.arcgis.com/}}
\end{enumerate}

\protect\hypertarget{id.7z0ipue2rriu}{}{}

\begin{enumerate}
\setcounter{enumi}{5}
\tightlist
\item
  {Satellite Sentinel 2 :
  }{\href{https://www.google.com/url?q=https://sentinel.esa.int/web/sentinel/missions/sentinel-2\&sa=D\&source=editors\&ust=1645818404108271\&usg=AOvVaw3rFXRo2--g1lc4MpypvX6h}{https://sentinel.esa.int/web/sentinel/missions/sentinel-2}}
\end{enumerate}

\protect\hypertarget{id.tageegyot90g}{}{}

\begin{enumerate}
\setcounter{enumi}{6}
\tightlist
\item
  {Spatial Resolution of SENTINEL-2 :
  }{\href{https://www.google.com/url?q=https://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-2-msi/resolutions/spatial\&sa=D\&source=editors\&ust=1645818404108754\&usg=AOvVaw3IKyvAI70M_x26lGii3RCP}{https://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-2-msi/resolutions/spatial}}
\end{enumerate}

\protect\hypertarget{id.njml8d1vx4cb}{}{}

\begin{enumerate}
\setcounter{enumi}{7}
\tightlist
\item
  {QGIS :
  }{\href{https://www.google.com/url?q=https://www.qgis.org/fr/site/\&sa=D\&source=editors\&ust=1645818404109161\&usg=AOvVaw1uvcwaWBA16R5pe7cjNQhN}{https://www.qgis.org/fr/site/}}
\end{enumerate}

\protect\hypertarget{id.6iopczr49dgf}{}{}

\begin{enumerate}
\setcounter{enumi}{8}
\tightlist
\item
  {Google Earth Engine :
  }{\href{https://www.google.com/url?q=https://earthengine.google.com/\&sa=D\&source=editors\&ust=1645818404109553\&usg=AOvVaw04dskWqcSaPAGoOK8YWMTC}{https://earthengine.google.com/}}
\end{enumerate}

\protect\hypertarget{id.uardra112ozy}{}{}

\begin{enumerate}
\setcounter{enumi}{9}
\tightlist
\item
  {Patchify :
  }{\href{https://www.google.com/url?q=https://pypi.org/project/patchify/\&sa=D\&source=editors\&ust=1645818404109926\&usg=AOvVaw2C0mZkdMLl8tE3XJINDNd6}{https://pypi.org/project/patchify/}}
\end{enumerate}

\protect\hypertarget{id.a7mbmw2twk35}{}{}

\begin{enumerate}
\setcounter{enumi}{10}
\tightlist
\item
  {Albumentation :
  }{\href{https://www.google.com/url?q=https://albumentations.ai/\&sa=D\&source=editors\&ust=1645818404110333\&usg=AOvVaw0zaUn9KTDJN6mu8fKQZuhR}{https://albumentations.ai/}}
\end{enumerate}

\protect\hypertarget{id.ipbhagqu1yjh}{}{}

\begin{enumerate}
\setcounter{enumi}{11}
\tightlist
\item
  {PSPNet :
  }{\href{https://www.google.com/url?q=https://arxiv.org/abs/1612.01105\&sa=D\&source=editors\&ust=1645818404110746\&usg=AOvVaw0g4kfBCjVZvJ-LcIdi5J9z}{https://arxiv.org/abs/1612.01105}}
\end{enumerate}

\protect\hypertarget{id.9fpcz1u0ap7f}{}{}

\begin{enumerate}
\setcounter{enumi}{12}
\tightlist
\item
  {GDAL/OGR :
  }{\href{https://www.google.com/url?q=https://gdal.org/\&sa=D\&source=editors\&ust=1645818404111207\&usg=AOvVaw1zaHrqGNfltmGQXx3b-PO_}{https://gdal.org/}}
\end{enumerate}

\protect\hypertarget{id.z1m1js6ahxf7}{}{}

\begin{enumerate}
\setcounter{enumi}{13}
\tightlist
\item
  {Rasterio :
  }{\href{https://www.google.com/url?q=https://rasterio.readthedocs.io/en/latest/\&sa=D\&source=editors\&ust=1645818404111617\&usg=AOvVaw1SsJh0Z6X-K0BPGxfKJh4R}{https://rasterio.readthedocs.io/en/latest/}}
\end{enumerate}

\begin{center}
\it \Large
Made with Google doc, build with \LaTeX
\end{center}

\end{document}
